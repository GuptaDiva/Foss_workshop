{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from langchain_community.document_loaders import PyPDFLoader\\nloader=PyPDFLoader('2022.acl-long.317.pdf')\\ndocs=loader.load()\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('2022.acl-long.317.pdf')\n",
    "docs=loader.load()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://fossunited.org/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\nFOSS United Foundation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tExplore\\n\\t\\n\\n\\n\\tContribute\\n\\n\\n\\tForum\\n\\n\\n\\tWiki\\n\\n\\n\\tBlog\\n\\n\\n\\tNewsletter\\n\\n\\n\\tGrants\\n\\n\\n\\tJobs\\n\\n\\n\\n\\n\\n\\t\\tEvents\\n\\t\\n\\n\\n\\n\\t\\tCity Communities\\n\\t\\n\\n\\n\\n\\t\\tFOSS Clubs\\n\\t\\n\\n\\n\\n\\t\\tPublic Policy\\n\\t\\n\\n\\n\\nLogin\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGrowing India's  FOSS Ecosystem\\n\\n\\n\\n\\n\\n          Explore Events\\n        \\n\\n          Join Forum\\n        \\n\\n\\n\\n\\n\\n\\n\\nABOUT\\n\\nFOSS United is a non-profit foundation that aims at promoting & strengthening the Free and Open Source Software (FOSS) ecosystem in India.\\nIndia is now a hub of startups, innovative consumer software, developer communities, and large scale technological infrastructure. However, somewhere along the lines, the spirit of FOSS and hacking seems to have been overshadowed. This is illustrated by the disproportionately low number of quality FOSS projects coming out of India given a thriving industry compared to the explosion of projects that has happened globally over the last decade. The foundation aims to provide grassroots support to FOSS projects and events, and evolve into a community-industry collaboration with a diverse group of members and patrons.\\n\\n\\n\\n\\n\\n\\n\\nGOALS\\n01\\n\\n\\nTo promote the spirit of hacking and tinkering.\\n\\n\\n\\n\\n\\nGOALS\\n02\\n\\n\\nTo build quality FOSS for public good.\\n\\n\\n\\n\\n\\nGOALS\\n03\\n\\n\\nTo evangelise the use of FOSS in different sectors.\\n\\n\\n\\n\\n\\n\\n\\n\\nPrograms\\n\\n\\n\\n\\n\\n\\nCity Communities\\n\\n\\nJoin a community in your city and take part in the FOSS Movement.\\n\\n\\n              View More\\n              \\n\\n\\n\\n\\n\\n\\n\\nFOSS Clubs \\n\\n\\nExplore our initiative that promotes FOSS culture among students.\\n\\n\\n              View More\\n              \\n\\n\\n\\n\\n\\n\\n\\nFOSS for Social Good\\n\\n\\nKnow more about OASIS coalition that promotes usage of FOSS in social sector.\\n\\n\\n              View More\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\nFlagship Events\\n\\n\\n\\nIndiaFOSS\\n\\n          IndiaFOSS is an annual FOSS conference, organised with passion and commitment by the FOSS\\n          United community.\\n        \\n\\n          View More\\n          \\n\\n\\n\\n\\nFOSS Hack\\n\\n          FOSS Hack is our flagship hackathon to promote FOSS by bringing enthusiasts together to\\n          build or extend FOSS projects.\\n        \\n\\n          View More\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\nUpcoming Events\\nView all Events ↗\\n\\n\\n\\n\\n\\nLUCKNOW\\n\\n\\n21 Dec 2024\\n\\nFOSS Meetup\\n\\n\\n\\nGoel Institute of Technolog...\\n\\n\\n\\n\\n\\n\\n\\nSHIVAMOGGA\\n\\n\\n23 Dec 2024\\n\\nDecember Meetup\\n\\n\\n\\nTo Be Announced\\n\\n\\n\\n\\n\\n\\n\\n\\n25 Dec 2024\\n\\nFOSS Overflow\\n\\n\\n\\nOnline\\n\\n\\n\\n\\n\\n\\n\\nMUMBAI\\n\\n\\n28 Dec 2024\\n\\nFOSS Meetup Mumbai\\n\\n\\n\\nRed Hat, Powai\\n\\n\\n\\n\\n\\n\\n\\nPUNE\\n\\n\\n04 Jan 2025\\n\\nFOSS Meetup Pune \\n\\n\\n\\nPeerlist, Pune\\n\\n\\n\\n\\n\\n\\n\\nKANPUR\\n\\n\\n11 Jan 2025\\n\\nFOSS Kanpur Meetup\\n\\n\\n\\nJagran Institute of Managem...\\n\\n\\n\\n\\n\\n\\n\\nHYDERABAD\\n\\n\\n11 Jan 2025\\n\\nFOSS Meetup Hyderabad\\n\\n\\n\\nTo Be Announced\\n\\n\\n\\n\\n\\n\\n\\n\\n19 Jan 2025\\n\\n\\n            Must Attend\\n          \\n\\nADCx India 25 ADCx India 25\\n\\n\\n\\nIndian Music Experience Mus...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Team\\nMeet the Workforce ↗\\n\\n\\n\\n\\n\\n\\n\\n Hari Prasanth S\\nCommunity Fellow\\n\\n\\n\\n\\n\\n\\n\\nAnsh Arora\\nProgram and Partnerships Manager\\n\\n\\n\\n\\n\\n\\n\\nHarsh Tandiya\\nSoftware Developer\\n\\n\\n\\n\\n\\n\\n\\nJeswin Jose\\nGraphic Designer\\n\\n\\n\\n\\n\\n\\n\\nPoruri Sai Rahul\\nCEO, Director\\n\\n\\n\\n\\n\\n\\n\\nRuchika Bagde\\nDiversity and Operations Manager\\n\\n\\n\\n\\n\\n\\n\\nVishal Arya\\nProgram Director\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIndustry Partnership\\nThe foundation was started as a collaboration between the teams at Frappe, the publisher of ERPNext, one of the largest FOSS projects out of India, and Zerodha. Currently we have two more partners supporting us, and we invite more organisations to join the movement.\\n\\nView Current Partners ↗\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNewsletter\\n\\n          Monthly digest of the latest news and updates about FOSS grants, events and mon school.\\n          Subscribe to stay updated and get news from us directly in your inbox.\\n        \\n\\n\\nSubscribe\\n\\n\\n\\n\\n\\nGet in Touch\\n\\n        Write to us at foundation@fossunited.org or post a message on the\\n        forum.\\n\\n        Join our Matrix Community or our\\n        Telegram Community. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          View Source Code\\n        \\n\\n\\n\\n\\nAbout\\n\\nTeam\\nIndustry Partners\\nPublic Policy Initiatives\\nMatrix Space\\nTelegram Group\\nForum\\nBranding\\n\\n\\n\\nPolicy\\n\\nPrivacy Policy\\nAccount Deletion\\nCode of Conduct\\nRefund Policy\\nTerms of Service\\n\\n\\n\\nSocial Media\\n\\nTwitter\\nYoutube\\nLinkedIn\\nMastodon\\nInstagram\\n\\n\\n\\nFlagship Event\\n\\nIndiaFOSS\\nFOSS Hack\\n\\n\\n\\n\\nFOSS United Foundation. CC-BY-SA.\\nCIN – U74999MH2016NPL288653.D-324 Neelkanth Business Park, Vidyavihar, Mumbai.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBuilt with Frappe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='FOSS United Foundation'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='Explore'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='Explore\\n\\t\\n\\n\\n\\tContribute\\n\\n\\n\\tForum\\n\\n\\n\\tWiki\\n\\n\\n\\tBlog\\n\\n\\n\\tNewsletter\\n\\n\\n\\tGrants\\n\\n\\n\\tJobs'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='Jobs\\n\\n\\n\\n\\n\\n\\t\\tEvents\\n\\t\\n\\n\\n\\n\\t\\tCity Communities\\n\\t\\n\\n\\n\\n\\t\\tFOSS Clubs\\n\\t\\n\\n\\n\\n\\t\\tPublic Policy\\n\\t\\n\\n\\n\\nLogin'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content=\"Login\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGrowing India's  FOSS Ecosystem\"),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='Explore Events\\n        \\n\\n          Join Forum\\n        \\n\\n\\n\\n\\n\\n\\n\\nABOUT'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='FOSS United is a non-profit foundation that aims at promoting & strengthening the Free and Open'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='the Free and Open Source Software (FOSS) ecosystem in India.'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='India is now a hub of startups, innovative consumer software, developer communities, and large'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='and large scale technological infrastructure. However, somewhere along the lines, the spirit of'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='the spirit of FOSS and hacking seems to have been overshadowed. This is illustrated by the'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='illustrated by the disproportionately low number of quality FOSS projects coming out of India given'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='out of India given a thriving industry compared to the explosion of projects that has happened'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='that has happened globally over the last decade. The foundation aims to provide grassroots support'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='grassroots support to FOSS projects and events, and evolve into a community-industry collaboration'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='collaboration with a diverse group of members and patrons.'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='GOALS\\n01\\n\\n\\nTo promote the spirit of hacking and tinkering.\\n\\n\\n\\n\\n\\nGOALS\\n02'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='GOALS\\n02\\n\\n\\nTo build quality FOSS for public good.\\n\\n\\n\\n\\n\\nGOALS\\n03'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='GOALS\\n03\\n\\n\\nTo evangelise the use of FOSS in different sectors.\\n\\n\\n\\n\\n\\n\\n\\n\\nPrograms'),\n",
       " Document(metadata={'source': 'https://fossunited.org/', 'title': 'FOSS United Foundation', 'language': 'en-US'}, page_content='Programs\\n\\n\\n\\n\\n\\n\\nCity Communities')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "doc=text_splitter.split_documents(docs)\n",
    "doc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "## load,chunk and index the content of the html page\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"leading-8 mt-7\",\"font-bold font-sans break-normal text-gray-900 dark:text-gray-100 text-1xl md:text-2xl pt-12\",\"font-bold font-sans break-normal text-gray-900 dark:text-gray-100 pt-6 pb-2 text-3xl md:text-4xl\")\n",
    "\n",
    "                     )))\n",
    "\n",
    "text_documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content='RAG Detective: Retrieval Augmented Generation with website dataAuthors: Ian Kelk, Alyssa Lutservitz, Nitesh Kumar, Mandy WongImage generated using OpenAI DALL-E 3 and edited in Figma by authorLarge language models (LLMs) like GPT-3.5 have proven to be capable when asked about commonly known subjects or topics that they would have received a large quantity of training data for. However, when asked about topics that include data they have not been trained on, they either state that they do not possess the knowledge or, worse, can hallucinate plausible answers.It is not usually possible to research company offerings using an LLM; in order to directly compare products and services, we need data that is more recent than what the model was trained on. The problem we wish to solve is to find a way to get up-to-date answers about companies that correspond to the information on their website.As well, in order to fulfill milestones for this course that would not otherwise be addressed, we'),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content='a way to get up-to-date answers about companies that correspond to the information on their website.As well, in order to fulfill milestones for this course that would not otherwise be addressed, we fine-tune a BERT model to perform financial sentiment analysis when GPT-3.5 reports that the response may be financial in nature.Proposed SolutionThere are two main ways of addressing this limitation: fine-tuning and retrieval augmented generation (RAG).Fine-tuning is the process of continuing to train the model using your own data with a significantly smaller learning rate. The newly-gained knowledge is then encapsulated in the model weights themselves. However, fine-tuning requires another copy of the model and the associated costs of hosting it, as well as the risk of \"catastrophic forgetting,\" where the model forgets previously learned information.RAG, however, makes use of a source of knowledge, typically a vector store of embeddings and their associated texts. By comparing the'),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content='where the model forgets previously learned information.RAG, however, makes use of a source of knowledge, typically a vector store of embeddings and their associated texts. By comparing the predicted embeddings of the query to the embeddings in the vector store, we can form a prompt for the LLM that fits inside its context and contains the information needed to answer the question.Our solution has three major components:A chatbot that uses scraped data from the website\\'ssitemap.xml file—a file intended to guide search engines to all scrapable links on the site—in a manner that\\'s more specific and insightful than using a search engine. The LLM should only use this context to answer the question and not insert its own training data or hallucinate an answer. This is simple to test with questions like \"Who is Kim Kardashian?\" which would be clearly known to the model, and ensure it replies that this answer \"is not within the context provided.\"A real-time scraper of websites on the'),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content='questions like \"Who is Kim Kardashian?\" which would be clearly known to the model, and ensure it replies that this answer \"is not within the context provided.\"A real-time scraper of websites on the application through asynchronous calls to the API.Financial sentiment analysis on relevant completions from the LLM. As part of the prompt for GPT-3.5, we ask if its response is financial in nature. If it says it is, then our fine-tuned BERT model is called and classifies the response, displaying a plot of the probabilities and an appropriately cute, totally non-copyright-infringing Bert puppet.The RAG componentTo understand RAG, let\\'s use a simplified analogy. There are two types of test questions a person might be asked; the first is a simple request for a fact:What is the capital of France?Who was the first person to climb Mount Everest?In what year did Canada gain independence from Great Britain?These questions don\\'t require any special skills to answer; someone with the right reference'),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content='was the first person to climb Mount Everest?In what year did Canada gain independence from Great Britain?These questions don\\'t require any special skills to answer; someone with the right reference material could just look up the correct response.For some tests, \"a textbook is all you need.\" Image generated using OpenAI DALL-E 3The other type of question is one where it doesn\\'t matter if you\\'re secretly hiding a textbook on the material; it involves a studied skill. If you haven\\'t studied and practiced, you won\\'t be able to respond in a satisfactory way. Some examples could be:Write a poem in German.Write a computer program to calculate the first million prime numbers.Compose a symphony in the style of BeethovenEven if there were a lot of mathematics texts on elliptic curves surrounding you, it\\'s unlikely that you could prove Fermat\\'s Last Theorem unless it was in one of the books verbatim!For others tests, all the books in the world can\\'t save you! Image generated using OpenAI DALL-E'),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content='it\\'s unlikely that you could prove Fermat\\'s Last Theorem unless it was in one of the books verbatim!For others tests, all the books in the world can\\'t save you! Image generated using OpenAI DALL-E 3In the world of large language models, RAG is used to solve the first type of problem, where it\\'s easy to cheat. Fine-tuning is used to solve the second, where a model would likely have to actually learn the material in order to solve it. RAG is easier—it doesn\\'t require retraining a model, you don\\'t have to deal with the internal works of a model, and you can adjust the data the model \"cheats\" off of rather easily. Interestingly, it also significantly reduces the amount a model \"hallucinates\" answers—a common issue for LLMs when they invent fictional but plausible answers based on insufficient training data. The only hard part of RAG is finding the relevant data to give a model; models have limits on how much they can be prompted with, and a 500-page history textbook is just too'),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content=\"training data. The only hard part of RAG is finding the relevant data to give a model; models have limits on how much they can be prompted with, and a 500-page history textbook is just too long.Here's the basics of how RAG works:Data Organization: Imagine you're the little guy in the cartoon above, surrounded by textbooks. We take each of these books and break them into bite-sized pieces—one might be about quantum physics, while another might be about space exploration. Each of these pieces, or documents, is processed to create a vector, which is like an address in the library that points right to that chunk of information.Vector Creation: Each of these chunks is passed through an embedding model, a type of model that creates a vector representation of hundreds or thousands of numbers that encapsulate the meaning of the information. The model assigns a unique vector to each chunk—sort of like creating a unique index that a computer can understand.Querying: When you want to ask an LLM\"),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content='encapsulate the meaning of the information. The model assigns a unique vector to each chunk—sort of like creating a unique index that a computer can understand.Querying: When you want to ask an LLM a question it may not have the answer to, you start by giving it a prompt, such as \"What\\'s the latest development in AI legislation?\"Retrieval: This prompt goes through an embedding model and transforms into a vector itself—it\\'s like it\\'s getting its own search terms based on its meaning and not just identical matches to its keywords. The system then uses this search term to scour the vector database for the most relevant chunks related to your question.Prepending the Context: The most relevant chunks are then served up as context. It\\'s similar to handing over reference material before asking your question, except we give the LLM a directive: \"Using this information, answer the following question.\" While the prompt to the LLM gets extended with a lot of this background information, you as a'),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content='question, except we give the LLM a directive: \"Using this information, answer the following question.\" While the prompt to the LLM gets extended with a lot of this background information, you as a user don\\'t see any of this. The complexity is handled behind the scenes.Answer Generation: Finally, equipped with this newfound information, the LLM generates a response that ties in the data it\\'s just retrieved, answering your question in a way that feels like it knew the answer all along.Image generated using OpenAI DALL-E 3 and comic bubbles and text by author using Comic Life 3To accomplish this goal, we built an application on top of a vector store called \"Weaviate\". We built a web scraper in Python that crawls a given website\\'s sitemap.xml, which is a listing of pages used to help search engines crawl the site. Due to the seemingly endless variability of internet websites, it turned out to be a bit of a challenge.The Scraper ComponentOur web scraper is built to handle many of the'),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content=\"engines crawl the site. Due to the seemingly endless variability of internet websites, it turned out to be a bit of a challenge.The Scraper ComponentOur web scraper is built to handle many of the challenges of modern web architecture, capture data often missed by conventional scraping methods, and stream the scraping activity in real time to our web application. This endpoint represents the vital data collection phase in our solution designed to fetch web data, transition it through various stages of storage and processing, and ultimately index it in the vector store for swift retrieval.The scraper uses the PythonBeautifulSoup library to sift through the HTML and CSS contents initially. However, some modern websites rely on JavaScript for dynamic content generation, creating a hurdle for standard scraping methodologies that rely purely on HTTP requests. The scraping system resolves this issue by employing Selenium WebDriver, a tool that simulates a real user's interaction with web\"),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content='standard scraping methodologies that rely purely on HTTP requests. The scraping system resolves this issue by employing Selenium WebDriver, a tool that simulates a real user\\'s interaction with web pages through a \"headless\" browser—that is, Google Chrome running without a graphical front end—that fully supports dynamic content loading. If the scraper\\'s initial efforts to extract data via direct HTTP requests are stymied or yield data below a set threshold, Selenium is engaged. This technique ensures that the scraper can successfully access content that would not be available through static page loads.The scraper also bypasses unnecessary elements, such as images, to mitigate overhead and facilitate faster processing. Once the data is collected, control is reverted back to BeautifulSoup to filter and extract the text from the HTML.Post-extraction, the data is serialized and then saved in Google Cloud Storage as CSV files, serving as a backup of the data for the vector store. The'),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content='to filter and extract the text from the HTML.Post-extraction, the data is serialized and then saved in Google Cloud Storage as CSV files, serving as a backup of the data for the vector store. The scraped data is then chunked and inserted into Weaviate using an orchestration library, LlamaIndexwhere it is added to the vector store index.\"Looks like someone took \\'Ctrl-X\\' a bit too literally!\" Image generated using OpenAI DALL-E 3OrchestrationLet\\'s look at what happens after the data is scraped in a bit more detail, as there is some orchestration to be done to insert it into the vector store. These steps were mentioned at an abstract level above, but more specifically, the steps involve:Taking each scraped website and breaking it into chunks. The lengths of these chunks have a profound impact on how well the retrieval process works.For each chunk, generate an embedding vector using OpenAI\\'s text-embedding-ada-002 model. Both Weaviate and LlamaIndex integrate this model natively.Insert'),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content=\"on how well the retrieval process works.For each chunk, generate an embedding vector using OpenAI's text-embedding-ada-002 model. Both Weaviate and LlamaIndex integrate this model natively.Insert this embedding vector and the text chunk into the Weaviate vector store.For the retrieval step when prompting the model:Take the prompt and put it through text-embedding-ada-002 to get an embedding.Using that embedding, find the chunks that should answer the prompt, then prepend them to the query and send it to GPT-3.5The actual chunking of the documents is somewhat of an art in itself. GPT-3.5 has a maximum context length of 4,096 tokens, or about 3,000 words. Those words represent the sum total of what the model can handle—if we create a prompt with a context 3,000 words long, the model will not have enough room to generate a response. Realistically, we shouldn't prompt with more than about 2,000 words for GPT-3.5. This means there is a trade-off with chunk size that is data-dependent.With\"),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content=\"have enough room to generate a response. Realistically, we shouldn't prompt with more than about 2,000 words for GPT-3.5. This means there is a trade-off with chunk size that is data-dependent.With smaller chunk_size values, the text returned produces more detailed chunks of text but risks missing information if they're located far away in the text. On the other hand, larger chunk_size values are more likely to include all necessary information in the top chunks, ensuring better response quality, but if the information is distributed throughout the text, it will miss important sections.Let's use some examples to illustrate how this trade-off works, using the recent Tesla Cybertruck release event. While some models of the truck will be available in 2024, the cheapest model—with just RWD—will not be available until 2025. Depending on the formatting and chunking of the text used for RAG, the model's response may or may not encounter this fact!In these images, blue indicates where a match\"),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content=\"not be available until 2025. Depending on the formatting and chunking of the text used for RAG, the model's response may or may not encounter this fact!In these images, blue indicates where a match was found and the chunk was returned; the grey box indicates the chunk was not retrieved; and the red text indicates where relevant text existed but was not retrieved. Let's take a look at an example where shorter chunks succeed:Exhibit A: Shorter chunks are better… sometimes. Background desk image generated using OpenAI DALL-E 3 and text by author using Pixelmator Pro.In the image above, on the left, the text is structured so that the admission that the RWD will be released in 2025 is separated by a paragraph but also has relevant text that is matched by the query. The method of retrieving two shorter chunks works better because it captures all the information. On the right, the retriever is only retrieving a single chunk and therefore does not have the room to return the additional\"),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content=\"two shorter chunks works better because it captures all the information. On the right, the retriever is only retrieving a single chunk and therefore does not have the room to return the additional information, and the model is given incorrect information.However, this isn't always the case; sometimes longer chunks work better when text that holds the true answer to the question doesn't strongly match the query. Here's an example where longer chunks succeed:Exhibit B: Longer chunks are better… sometimes. Background desk image generated using OpenAI DALL-E 3 and text by author using Pixelmator Pro.After some experimentation, we opted to use chunks 1,000 tokens long and retrieve two of them to prompt GPT-3.5. Since the GPT-3.5 can handle a 4,096 context length, that should leave plenty of space for an appropriate response.When we first began the project, we were doing the chunking, indexing, and retrieval ourselves, and it worked just fine. There was a learning curve to learning GraphQL,\"),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content=\"for an appropriate response.When we first began the project, we were doing the chunking, indexing, and retrieval ourselves, and it worked just fine. There was a learning curve to learning GraphQL, which is what Weaviate uses for a query language.An example of the GraphQL we had to use to test if a previous website and timestamp had already been inserted into the vector store.The appeal of using a library like LlamaIndex is that it abstracts away this orchestration, allowing us to swap out other vector stores if we want to (Weaviate has many competitors in the space, such as Milvus, Qdrant, Pinecone, and others emerging all the time). Using LlamaIndex also allows us to experiment later with more complex RAG implementations, such as tree-structured data and recursive prompting. However, using such a new library came with its own share of challenges, specifically the lack of proper documentation. The vast majority of their help resources were examples, and if those examples didn't fit\"),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content=\"such a new library came with its own share of challenges, specifically the lack of proper documentation. The vast majority of their help resources were examples, and if those examples didn't fit our use case, there was no recourse other than asking the developers on Discord or reading the source code ourselves.The BERT ComponentTo fulfill the requirements for model hosting for the course, we fine-tuned a BERT model, trained it, and hosted it via a pipeline on Google Vertex. By adding this model to our application, we can actually prompt GPT-3.5 to return a flag along with its response and let us know if it thinks the answer it's giving is financial in nature. When it does, we can display an appropriately comical, non-copyright-infringing Bert puppet and also a plot of the probabilities returned by the model.The process of refining the BERT model's training was as much about the data as it was about the technical configuration. The goal was to teach the model to discern financial\"),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content=\"returned by the model.The process of refining the BERT model's training was as much about the data as it was about the technical configuration. The goal was to teach the model to discern financial sentiments within texts—a skill useful for anyone needing insights from complex financial news. Since sentiments in such articles can be subtle and not immediately apparent, fine-tuning a specialized model was essential to providing laypersons with clear indicators of the underlying sentiment.To train our BERT model, we used thefinancial_phrasebank dataset, which is composed of sentences labeled with sentiments by individuals well-versed in finance. However, an intriguing issue arises with such a dataset: variance in the levels of agreement among annotators, which can implicitly influence the model's learning and its subsequent predictions.When we fine-tuned the BERT model using these datasets—which represent 100%, 75%, 66%, and 50% annotator agreement—it seemed pretty clear that the more\"),\n",
       " Document(metadata={'source': 'https://freedium.cfd/https://medium.com/@iankelk/rag-detective-retrieval-augmented-generation-with-website-data-5a748b063040'}, page_content=\"model's learning and its subsequent predictions.When we fine-tuned the BERT model using these datasets—which represent 100%, 75%, 66%, and 50% annotator agreement—it seemed pretty clear that the more the annotators agreed, the better the trained model.When just using the various datasets, the more agreement, the better the model performance. Screenshot from Weights & Biases platform.Suspicious right? What if the models weren't actually better with higher consensus, but instead, a higher level of consensus implies that the financial statements are just... easier to classify?It's not hard to imagine that higher consensus might skew the model towards recognizing only clear-cut sentiments while neglecting those that are more nuanced, which are oftentimes the reality in complex financial texts.To address this, we set out to de-bias our data. We aimed to construct a training and testing split that would give a fair representation of all sentiment clarity levels, ensuring the model's utility\")]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(text_documents)\n",
    "documents[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents,OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not be available until 2025. Depending on the formatting and chunking of the text used for RAG, the model's response may or may not encounter this fact!In these images, blue indicates where a match was found and the chunk was returned; the grey box indicates the chunk was not retrieved; and the red text indicates where relevant text existed but was not retrieved. Let's take a look at an example where shorter chunks succeed:Exhibit A: Shorter chunks are better… sometimes. Background desk image generated using OpenAI DALL-E 3 and text by author using Pixelmator Pro.In the image above, on the left, the text is structured so that the admission that the RWD will be released in 2025 is separated by a paragraph but also has relevant text that is matched by the query. The method of retrieving two shorter chunks works better because it captures all the information. On the right, the retriever is only retrieving a single chunk and therefore does not have the room to return the additional\n"
     ]
    }
   ],
   "source": [
    "query = \"Need of RAG\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000023CDECB6D20>, search_kwargs={})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb__=FAISS.from_documents(documents,OllamaEmbeddings())\n",
    "retriever__=vectordb__.as_retriever()\n",
    "retriever__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb__.save_local(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.load_local(\"data\", OllamaEmbeddings(), allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "## Load Ollama LAMA2 LLM model\n",
    "llm=Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Summarize the text\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000023CDECB6D20>, search_kwargs={})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectordb__.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"Summarize the given text.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this project, the author and their team trained and fine-tuned a BERT model for sentiment analysis in financial news articles. They used a dataset with varying levels of annotator agreement, which led to interesting results. The team built an application on top of a vector store called \"Weaviate\" to generate answers to questions using the fine-tuned BERT model. The project involved modern solutions and technologies such as cloud computing, RAG architecture, web scraping in Python, and more. The author notes that while some questions can be answered with a simple reference material, others require studied skills and practice to provide satisfactory responses.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
